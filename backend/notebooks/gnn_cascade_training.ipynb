{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Cascade Failure Prediction - Production Scale\n",
    "\n",
    "## Engineering: PyTorch Geometric GCN for Grid Resilience\n",
    "\n",
    "This notebook implements a production-grade Graph Convolutional Network (GCN) for cascade failure prediction,\n",
    "designed to run on Snowpark Container Services with GPU acceleration.\n",
    "\n",
    "**Key Features:**\n",
    "- 3-layer GCN architecture (10 → 64 → 64 → 32 → 1)\n",
    "- BFS-based cascade ordering with wave depth labels\n",
    "- Graph centrality features integration\n",
    "- Snowflake ML model registry integration\n",
    "\n",
    "**Based on:** Original GNN demo at `/Documents/gnn_resilient_energy_digital_twin/notebooks/grid_cascade_analysis.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install torch torch-geometric snowflake-ml-python pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.registry import Registry\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Snowflake and Load Grid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowflake connection\n",
    "connection_name = os.getenv(\"SNOWFLAKE_CONNECTION_NAME\", \"cpe_demo_CLI\")\n",
    "session = Session.builder.config(\"connection_name\", connection_name).create()\n",
    "\n",
    "print(f\"Connected to: {session.get_current_account()}\")\n",
    "print(f\"Database: {session.get_current_database()}\")\n",
    "print(f\"Warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid nodes with centrality features\n",
    "nodes_query = \"\"\"\n",
    "SELECT \n",
    "    n.NODE_ID,\n",
    "    n.NODE_TYPE,\n",
    "    n.LAT,\n",
    "    n.LON,\n",
    "    n.CAPACITY_KW,\n",
    "    n.VOLTAGE_KV,\n",
    "    n.CRITICALITY_SCORE,\n",
    "    n.DOWNSTREAM_TRANSFORMERS,\n",
    "    n.DOWNSTREAM_CAPACITY_KVA,\n",
    "    COALESCE(c.DEGREE_CENTRALITY, 0) AS DEGREE_CENTRALITY,\n",
    "    COALESCE(c.NORMALIZED_DEGREE, 0) AS NORMALIZED_DEGREE,\n",
    "    COALESCE(c.REACH_EXPANSION_RATIO, 0) AS REACH_EXPANSION_RATIO,\n",
    "    COALESCE(c.NEIGHBORS_1HOP, 0) AS NEIGHBORS_1HOP,\n",
    "    COALESCE(c.NEIGHBORS_2HOP, 0) AS NEIGHBORS_2HOP\n",
    "FROM SI_DEMOS.ML_DEMO.GRID_NODES n\n",
    "LEFT JOIN SI_DEMOS.CASCADE_ANALYSIS.NODE_CENTRALITY_FEATURES c ON n.NODE_ID = c.NODE_ID\n",
    "WHERE n.LAT IS NOT NULL AND n.LON IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "nodes_df = session.sql(nodes_query).to_pandas()\n",
    "print(f\"Loaded {len(nodes_df)} nodes\")\n",
    "nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grid edges\n",
    "edges_query = \"\"\"\n",
    "SELECT \n",
    "    EDGE_ID,\n",
    "    FROM_NODE,\n",
    "    TO_NODE,\n",
    "    EDGE_TYPE,\n",
    "    DISTANCE_KM,\n",
    "    IMPEDANCE_PU\n",
    "FROM SI_DEMOS.ML_DEMO.GRID_EDGES\n",
    "\"\"\"\n",
    "\n",
    "edges_df = session.sql(edges_query).to_pandas()\n",
    "print(f\"Loaded {len(edges_df)} edges\")\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build PyTorch Geometric Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node ID to index mapping\n",
    "node_id_to_idx = {node_id: idx for idx, node_id in enumerate(nodes_df['NODE_ID'])}\n",
    "print(f\"Node ID mapping created: {len(node_id_to_idx)} nodes\")\n",
    "\n",
    "# Filter edges to only include nodes in our dataset\n",
    "valid_edges = edges_df[\n",
    "    edges_df['FROM_NODE'].isin(node_id_to_idx) & \n",
    "    edges_df['TO_NODE'].isin(node_id_to_idx)\n",
    "]\n",
    "print(f\"Valid edges: {len(valid_edges)} / {len(edges_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build edge index tensor (COO format)\n",
    "source_nodes = [node_id_to_idx[n] for n in valid_edges['FROM_NODE']]\n",
    "target_nodes = [node_id_to_idx[n] for n in valid_edges['TO_NODE']]\n",
    "\n",
    "# Add reverse edges for undirected graph\n",
    "edge_index = torch.tensor(\n",
    "    [source_nodes + target_nodes, target_nodes + source_nodes],\n",
    "    dtype=torch.long\n",
    ")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build node feature matrix (10 features)\n",
    "feature_columns = [\n",
    "    'CAPACITY_KW', 'VOLTAGE_KV', 'CRITICALITY_SCORE',\n",
    "    'DOWNSTREAM_TRANSFORMERS', 'DOWNSTREAM_CAPACITY_KVA',\n",
    "    'DEGREE_CENTRALITY', 'NORMALIZED_DEGREE', 'REACH_EXPANSION_RATIO',\n",
    "    'NEIGHBORS_1HOP', 'NEIGHBORS_2HOP'\n",
    "]\n",
    "\n",
    "# Fill NaN with 0 and normalize features\n",
    "features = nodes_df[feature_columns].fillna(0).values\n",
    "features_normalized = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)\n",
    "\n",
    "x = torch.tensor(features_normalized, dtype=torch.float32)\n",
    "print(f\"Node features shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cascade failure labels using BFS from high-criticality nodes\n",
    "from collections import deque\n",
    "\n",
    "def simulate_cascade(start_idx, adjacency, max_depth=5):\n",
    "    \"\"\"BFS cascade simulation returning wave depths\"\"\"\n",
    "    wave_depths = {start_idx: 0}\n",
    "    queue = deque([start_idx])\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        current_depth = wave_depths[current]\n",
    "        \n",
    "        if current_depth >= max_depth:\n",
    "            continue\n",
    "            \n",
    "        if current in adjacency:\n",
    "            for neighbor in adjacency[current]:\n",
    "                if neighbor not in wave_depths:\n",
    "                    wave_depths[neighbor] = current_depth + 1\n",
    "                    queue.append(neighbor)\n",
    "    \n",
    "    return wave_depths\n",
    "\n",
    "# Build adjacency list\n",
    "adjacency = {}\n",
    "for i in range(edge_index.shape[1]):\n",
    "    src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
    "    if src not in adjacency:\n",
    "        adjacency[src] = []\n",
    "    adjacency[src].append(dst)\n",
    "\n",
    "print(f\"Adjacency list built for {len(adjacency)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cascade labels from top 10 highest criticality nodes\n",
    "high_crit_indices = nodes_df['CRITICALITY_SCORE'].fillna(0).nlargest(10).index.tolist()\n",
    "print(f\"High criticality Patient Zero candidates: {high_crit_indices[:5]}...\")\n",
    "\n",
    "# Aggregate cascade labels from multiple starting points\n",
    "cascade_labels = np.zeros(len(nodes_df))\n",
    "for start_idx in high_crit_indices:\n",
    "    wave_depths = simulate_cascade(start_idx, adjacency, max_depth=3)\n",
    "    for node_idx, depth in wave_depths.items():\n",
    "        # Label as 1 if affected in any cascade simulation\n",
    "        cascade_labels[node_idx] = 1\n",
    "\n",
    "y = torch.tensor(cascade_labels, dtype=torch.float32)\n",
    "print(f\"Cascade labels: {y.sum().item():.0f} affected / {len(y)} total ({100*y.mean().item():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Geometric Data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "print(f\"Graph Data:\")\n",
    "print(f\"  - Nodes: {data.num_nodes}\")\n",
    "print(f\"  - Edges: {data.num_edges}\")\n",
    "print(f\"  - Node features: {data.num_node_features}\")\n",
    "print(f\"  - Is directed: {data.is_directed()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define GCN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer Graph Convolutional Network for Cascade Failure Prediction\n",
    "    \n",
    "    Architecture: 10 → 64 → 64 → 32 → 1\n",
    "    - Based on original GNN demo\n",
    "    - BatchNorm for training stability\n",
    "    - Dropout for regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features=10, hidden_dim=64, dropout=0.3):\n",
    "        super(CascadeGCN, self).__init__()\n",
    "        \n",
    "        # GCN layers\n",
    "        self.conv1 = GCNConv(in_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, 32)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1: 10 → 64\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2: 64 → 64\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 3: 64 → 32\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Output: 32 → 1 (cascade probability)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x).squeeze(-1)\n",
    "    \n",
    "    def predict_cascade_risk(self, x, edge_index):\n",
    "        \"\"\"Get cascade risk scores for all nodes\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x, edge_index)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CascadeGCN(in_features=10, hidden_dim=64, dropout=0.3).to(device)\n",
    "print(f\"Model on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val/test split (node-level)\n",
    "num_nodes = data.num_nodes\n",
    "indices = np.arange(num_nodes)\n",
    "\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.15, random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "print(f\"Train: {train_mask.sum().item()} | Val: {val_mask.sum().item()} | Test: {test_mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "# Class imbalance handling\n",
    "pos_weight = (1 - y.mean()) / y.mean()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "\n",
    "print(f\"Positive class weight: {pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "patience = 30\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(data.x, data.edge_index)\n",
    "    train_loss = F.binary_cross_entropy(out[train_mask.to(device)], data.y[train_mask.to(device)])\n",
    "    \n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        val_loss = F.binary_cross_entropy(out[val_mask.to(device)], data.y[val_mask.to(device)])\n",
    "    \n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_cascade_gcn.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete. Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_cascade_gcn.pt'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(data.x, data.edge_index)\n",
    "    \n",
    "# Test metrics\n",
    "y_test = data.y[test_mask.to(device)].cpu().numpy()\n",
    "y_pred = predictions[test_mask.to(device)].cpu().numpy()\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "ap_score = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  - AUC-ROC: {auc_score:.4f}\")\n",
    "print(f\"  - Average Precision: {ap_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top Patient Zero candidates\n",
    "cascade_risk_scores = predictions.cpu().numpy()\n",
    "nodes_df['GNN_CASCADE_RISK'] = cascade_risk_scores\n",
    "\n",
    "top_risk_nodes = nodes_df.nlargest(20, 'GNN_CASCADE_RISK')[[\n",
    "    'NODE_ID', 'NODE_TYPE', 'CRITICALITY_SCORE', 'GNN_CASCADE_RISK',\n",
    "    'DEGREE_CENTRALITY', 'DOWNSTREAM_TRANSFORMERS'\n",
    "]]\n",
    "\n",
    "print(\"\\nTop 20 Patient Zero Candidates (GNN-based):\")\n",
    "top_risk_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Register Model in Snowflake ML Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class for Snowflake ML Registry\n",
    "class CascadeGCNPredictor:\n",
    "    \"\"\"Wrapper for Snowflake ML Registry deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, model_state_dict, node_id_mapping, feature_columns):\n",
    "        self.model = CascadeGCN(in_features=10, hidden_dim=64)\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        self.model.eval()\n",
    "        self.node_id_mapping = node_id_mapping\n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "    def predict(self, df):\n",
    "        \"\"\"Predict cascade risk for nodes in dataframe\"\"\"\n",
    "        # This would be adapted for batch inference in SPCS\n",
    "        pass\n",
    "\n",
    "# Save model artifacts\n",
    "model_artifacts = {\n",
    "    'state_dict': model.state_dict(),\n",
    "    'node_id_mapping': node_id_to_idx,\n",
    "    'feature_columns': feature_columns,\n",
    "    'architecture': {\n",
    "        'in_features': 10,\n",
    "        'hidden_dim': 64,\n",
    "        'dropout': 0.3\n",
    "    },\n",
    "    'metrics': {\n",
    "        'auc_roc': auc_score,\n",
    "        'average_precision': ap_score\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(model_artifacts, 'cascade_gcn_full.pt')\n",
    "print(\"Model artifacts saved to cascade_gcn_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register in Snowflake ML Registry\n",
    "try:\n",
    "    registry = Registry(session=session, database_name=\"SI_DEMOS\", schema_name=\"ML_DEMO\")\n",
    "    \n",
    "    # Log model to registry\n",
    "    mv = registry.log_model(\n",
    "        model_name=\"CASCADE_GCN_MODEL\",\n",
    "        version_name=\"v1_gcn_3layer\",\n",
    "        model=model,\n",
    "        conda_dependencies=[\"pytorch\", \"torch_geometric\"],\n",
    "        comment=\"3-layer GCN for cascade failure prediction. AUC-ROC: {:.4f}\".format(auc_score),\n",
    "        metrics={\n",
    "            \"auc_roc\": auc_score,\n",
    "            \"average_precision\": ap_score,\n",
    "            \"num_nodes\": len(nodes_df),\n",
    "            \"num_edges\": len(valid_edges),\n",
    "            \"num_features\": 10\n",
    "        }\n",
    "    )\n",
    "    print(f\"Model registered: {mv.model_name}.{mv.version_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: ML Registry registration skipped: {e}\")\n",
    "    print(\"Model saved locally as cascade_gcn_full.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Predictions to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions table\n",
    "predictions_df = nodes_df[['NODE_ID', 'NODE_TYPE', 'GNN_CASCADE_RISK', 'CRITICALITY_SCORE']].copy()\n",
    "predictions_df['PREDICTION_TIMESTAMP'] = pd.Timestamp.now()\n",
    "\n",
    "# Write to Snowflake\n",
    "try:\n",
    "    snowpark_df = session.create_dataframe(predictions_df)\n",
    "    snowpark_df.write.mode(\"overwrite\").save_as_table(\"SI_DEMOS.CASCADE_ANALYSIS.GNN_PREDICTIONS\")\n",
    "    print(f\"Predictions written to SI_DEMOS.CASCADE_ANALYSIS.GNN_PREDICTIONS\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not write to Snowflake: {e}\")\n",
    "    predictions_df.to_csv('gnn_predictions.csv', index=False)\n",
    "    print(\"Predictions saved to gnn_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"GNN CASCADE FAILURE PREDICTION - TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel Architecture: 10 → 64 → 64 → 32 → 1 (GCN)\")\n",
    "print(f\"Training Nodes: {train_mask.sum().item():,}\")\n",
    "print(f\"Test AUC-ROC: {auc_score:.4f}\")\n",
    "print(f\"Test Average Precision: {ap_score:.4f}\")\n",
    "print(f\"\\nTop Patient Zero Candidates:\")\n",
    "for i, row in top_risk_nodes.head(5).iterrows():\n",
    "    print(f\"  - {row['NODE_ID']}: Risk={row['GNN_CASCADE_RISK']:.3f}, Type={row['NODE_TYPE']}\")\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\"  - Model: cascade_gcn_full.pt\")\n",
    "print(\"  - Predictions: SI_DEMOS.CASCADE_ANALYSIS.GNN_PREDICTIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
